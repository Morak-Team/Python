# ğŸ“Œ main.py
# ğŸ“Œ main.py

from crawlers.crawler_bss import run_bss_crawling
from crawlers.crawler_sehub import run_sehub_crawling
from crawlers.crawler_seis import run_seis_crawling
from crawlers.crawler_mybiz import run_mybiz_crawling
from openAPI.bizinfo_openAPI import fetch_bizinfo_data  # ğŸ”¥ ì¶”ê°€! (API ë¶ˆëŸ¬ì˜¤ê¸°)

def remove_duplicates(data):
    seen = set()
    unique_data = []
    for item in data:
        # ğŸ”¥ ê³µë°± ì œê±°í•œ ì œëª© ì• 7ê¸€ì ì¶”ì¶œ
        title_no_space = item['ê³µê³  ì œëª©'].replace(" ", "")  # ê³µë°± ì œê±°
        title_prefix = title_no_space[:7]  # ì• 7ê¸€ì ìë¥´ê¸°
        key = (title_prefix, item['ì—°ê²° ë§í¬'])
        if key not in seen:
            seen.add(key)
            unique_data.append(item)
    return unique_data


def main():
    print("âœ… í¬ë¡¤ë§ ì‹œì‘!")

    all_results = []

    # ê° ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰
    all_results.extend(run_bss_crawling())
    all_results.extend(run_sehub_crawling())
    all_results.extend(run_seis_crawling())
    all_results.extend(run_mybiz_crawling())
    all_results.extend(fetch_bizinfo_data())  # ğŸ”¥ APIë„ í•¨ê»˜ ì‹¤í–‰í•´ì„œ ê²°ê³¼ í•©ì¹˜ê¸°

    print(f"ğŸ”µ ì´ ìˆ˜ì§‘ëœ ë°ì´í„°: {len(all_results)}ê±´")

    # ì¤‘ë³µ ì œê±°
    final_results = remove_duplicates(all_results)

    print(f"ğŸŸ¢ ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ë°ì´í„°: {len(final_results)}ê±´")

    # ìµœì¢… ì¶œë ¥
    for idx, item in enumerate(final_results, 1):
        print(f"[{idx}] {item}")

if __name__ == "__main__":
    main()
