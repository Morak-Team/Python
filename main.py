# ğŸ“Œ main.py

from crawlers.crawler_bss import run_bss_crawling
from crawlers.crawler_sehub import run_sehub_crawling
from crawlers.crawler_seis import run_seis_crawling
from crawlers.crawler_mybiz import run_mybiz_crawling
from openAPI.bizinfo_openAPI import fetch_bizinfo_data

import requests
import os
from dotenv import load_dotenv
import pymysql

# âœ… í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()

# âœ… ë””ìŠ¤ì½”ë“œ ì•Œë¦¼ í•¨ìˆ˜
def send_discord_notification(message):
    webhook_url = os.getenv("DISCORD_WEBHOOK_URL")
    if not webhook_url:
        print("âŒ ë””ìŠ¤ì½”ë“œ ì›¹í›… URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    payload = {
        "content": message
    }

    try:
        response = requests.post(webhook_url, json=payload)
        if response.status_code == 204:
            print("âœ… ë””ìŠ¤ì½”ë“œ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
        else:
            print(f"âŒ ë””ìŠ¤ì½”ë“œ ì „ì†¡ ì‹¤íŒ¨: {response.status_code} {response.text}")
    except Exception as e:
        print(f"âŒ ë””ìŠ¤ì½”ë“œ ìš”ì²­ ì˜¤ë¥˜: {e}")

# âœ… ì¤‘ë³µ ì œê±° í•¨ìˆ˜
def remove_duplicates(data):
    seen = set()
    unique_data = []
    for item in data:
        title_no_space = item['ê³µê³  ì œëª©'].replace(" ", "")
        title_prefix = title_no_space[:7]
        key = (title_prefix, item['ì—°ê²° ë§í¬'])
        if key not in seen:
            seen.add(key)
            unique_data.append(item)
    return unique_data

# âœ… DBì— ì €ì¥ í•¨ìˆ˜
def save_to_db(data):
    db = pymysql.connect(
        host=os.getenv("MYSQL_HOST"),
        user=os.getenv("MYSQL_USER"),
        password=os.getenv("MYSQL_PASSWORD"),
        database=os.getenv("MYSQL_DATABASE"),
        charset="utf8mb4"
    )
    cursor = db.cursor()

    try:
        # ğŸ”¥ í…Œì´ë¸” ë¹„ìš°ê¸°
        cursor.execute("TRUNCATE TABLE support_announcements")
        db.commit()
        print("âœ… support_announcements í…Œì´ë¸” ì´ˆê¸°í™” ì™„ë£Œ")

        # ğŸ”¥ ë°ì´í„° ì‚½ì…
        insert_query = """
        INSERT INTO support_announcements 
        (title, organization, start_date, end_date, announcement_type, summary, link)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
        """

        for item in data:
            cursor.execute(insert_query, (
                item.get("ê³µê³  ì œëª©"),
                item.get("ì£¼ê´€ê¸°ê´€"),
                item.get("ì‹ ì²­ ì‹œì‘ì¼"),
                item.get("ì‹ ì²­ ì¢…ë£Œì¼"),
                item.get("ê³µê³  ìœ í˜•"),
                item.get("ìƒì„¸ ë‚´ìš©"),
                item.get("ì—°ê²° ë§í¬")
            ))
        db.commit()
        print("âœ… ëª¨ë“  ë°ì´í„° ì €ì¥ ì™„ë£Œ")

    except Exception as e:
        db.rollback()
        print(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        raise e

    finally:
        cursor.close()
        db.close()

# âœ… ë©”ì¸ í•¨ìˆ˜
def main():
    print("âœ… í¬ë¡¤ë§ ì‹œì‘!")
    all_results = []

    try:
        all_results.extend(run_bss_crawling())
        all_results.extend(run_sehub_crawling())
        all_results.extend(run_seis_crawling())
        all_results.extend(run_mybiz_crawling())
        all_results.extend(fetch_bizinfo_data())

        print(f"ğŸ”µ ì´ ìˆ˜ì§‘ëœ ë°ì´í„°: {len(all_results)}ê±´")

        final_results = remove_duplicates(all_results)
        print(f"ğŸŸ¢ ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ë°ì´í„°: {len(final_results)}ê±´")

        # ğŸ”¥ DB ì €ì¥
        save_to_db(final_results)

        # ğŸ”¥ ë””ìŠ¤ì½”ë“œ ì„±ê³µ ì•Œë¦¼
        send_discord_notification(f"âœ… í¬ë¡¤ë§ ë° DB ì €ì¥ ì™„ë£Œ! ({len(final_results)}ê±´ ìˆ˜ì§‘ë¨)")

    except Exception as e:
        print(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
        # ğŸ”¥ ë””ìŠ¤ì½”ë“œ ì‹¤íŒ¨ ì•Œë¦¼
        send_discord_notification(f"âŒ ì˜¤ëŠ˜ í¬ë¡¤ë§ ë˜ëŠ” DB ì €ì¥ ì‹¤íŒ¨!\n{str(e)}")

if __name__ == "__main__":
    main()
